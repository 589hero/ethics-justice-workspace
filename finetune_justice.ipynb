{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "spare-nursery",
   "metadata": {},
   "source": [
    "# This notebook learns and evaluates Justice Eater among the ETHICS datasets.\n",
    "\n",
    "# Import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dutch-shower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.3.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sacred-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.trainer_utils import set_seed\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "middle-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incorporated-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "happy-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EthicsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, csv_path, max_length=64):\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        self.scenarios = df['scenario'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.encodings = tokenizer(self.scenarios,\n",
    "                                   max_length=max_length,\n",
    "                                   padding='max_length',\n",
    "                                   truncation=True)\n",
    "        self.num_labels = len(set(self.labels))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.Tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def get_num_labels(self):\n",
    "        return self.num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-michigan",
   "metadata": {},
   "source": [
    "# Load Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amber-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-cooler",
   "metadata": {},
   "source": [
    "Set the values required for training.\n",
    "\n",
    "The value was set by referring to the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cloudy-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 16\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-administrator",
   "metadata": {},
   "source": [
    "# Create training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "atmospheric-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './ethics'\n",
    "train_name = 'justice/justice_train.csv'\n",
    "test_name = 'justice/justice_test.csv'\n",
    "test_hard_name = 'justice/justice_test_hard.csv'\n",
    "\n",
    "train_dataset = EthicsDataset(tokenizer, os.path.join(base_dir, train_name))\n",
    "test_dataset = EthicsDataset(tokenizer, os.path.join(base_dir, test_name))\n",
    "test_hard_dataset = EthicsDataset(tokenizer, os.path.join(base_dir, test_hard_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vietnamese-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_hard_loader = DataLoader(test_hard_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-canal",
   "metadata": {},
   "source": [
    "# Load bert-base model and optimizer for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "incredible-castle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = train_dataset.get_num_labels()\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-remainder",
   "metadata": {},
   "source": [
    "# Let's start Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "early-breakdown",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1362 [00:00<?, ?step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Epoch 1/2 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362/1362 [04:46<00:00,  4.76step/s, loss=0.3306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.5042\n",
      "Test Dataset\n",
      "Accuracy: 0.7352, Exact match: 0.1967\n",
      "Test Hard Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1362 [00:00<?, ?step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5682, Exact match: 0.0429\n",
      "< Epoch 2/2 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362/1362 [04:46<00:00,  4.75step/s, loss=0.3265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.3325\n",
      "Test Dataset\n",
      "Accuracy: 0.7548, Exact match: 0.2249\n",
      "Test Hard Dataset\n",
      "Accuracy: 0.5819, Exact match: 0.0526\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def train_epoch(train_loader, model, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_length = len(train_loader.dataset)\n",
    "    \n",
    "    with tqdm(total=len(train_loader), unit='step') as t:\n",
    "        for batch in train_loader:\n",
    "            inputs = {k: v.to(device).long() for k, v in batch.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss * len(batch['input_ids'])\n",
    "            \n",
    "            t.set_postfix(loss=f\"{loss:.4f}\")\n",
    "            t.update(1)\n",
    "    \n",
    "    loss = total_loss / total_length\n",
    "    \n",
    "    print(f\"Train Loss : {loss:.4f}\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    cors = []\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        inputs = {k: v.to(device).long() for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "\n",
    "        labels = inputs['labels'].detach().cpu().numpy()\n",
    "\n",
    "        cors += list(predictions == labels)\n",
    "            \n",
    "\n",
    "    acc = np.mean(cors)\n",
    "    em_sums = [int(cors[4*i]) + int(cors[4*i+1]) + int(cors[4*i+2]) + int(cors[4*i+3]) for i in range(len(cors) // 4)]\n",
    "    em_cors = [em_sums[i] == 4 for i in range(len(em_sums))]\n",
    "    em = np.mean(em_cors)\n",
    "    \n",
    "    print(f'Accuracy: {acc:.4f}, Exact match: {em:.4f}')\n",
    "    \n",
    "    results = {\n",
    "        'acc': acc,\n",
    "        'em': em,\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "    \n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    print(f'< Epoch {epoch+1}/{epochs} >')\n",
    "    \n",
    "    # train\n",
    "    train_epoch(train_loader, model, optimizer)\n",
    "    \n",
    "    # evaluate\n",
    "    print('Test Dataset')\n",
    "    test_results = evaluate(model, test_loader)\n",
    "    print('Test Hard Dataset')\n",
    "    test_hard_results = evaluate(model, test_hard_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-blackjack",
   "metadata": {},
   "source": [
    "# Save tokenizer and fine-tuned model to local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unsigned-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('./bert-base-uncased-justice')\n",
    "model.save_pretrained('./bert-base-uncased-justice')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
